# First we get our data.
mydata <- read.table("panel80.txt")
# attach(mydata) # In case you want to work with the variables directly
names(mydata) # This shows us all the variable names.
# options(scipen=20) # suppress "scientific" notation
options(scipen=NULL) # Brings things back to normal
reagan.model <- lm(REAFEEL3 ~ INC + AGE + PARTYID, data=mydata) 
summary(reagan.model)
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(reagan.model) # These are diagnostic plots.
windows()
carter.model <- lm(CARFEEL3 ~ INC + AGE + PARTYID, data=mydata)
summary(carter.model)
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(carter.model) # These are diagnostic plots.
mysubsetdata<-subset(mydata, select=c(REAFEEL3, CARFEEL3, INC, AGE, PARTYID)) #This keeps only the variables that we are using.
cor(mysubsetdata, use = "pairwise.complete.obs") # A correlation matrix for the variables in the regression
cov(mysubsetdata, use = "pairwise.complete.obs") # A covariance matrix for the variables in the regression
# Now let's get the standardized regression coeficients
sdvariables <- sapply(mysubsetdata, sd, na.rm = TRUE) # This gets the standard deviations of all the variables.
sdvariables # This prints out the standard deviations, which is not very useful but nice to see.
mystandardizeddata <- as.data.frame(scale(mysubsetdata, center=FALSE, scale=sdvariables) ) # standardize variables 
var(mystandardizeddata, use = "pairwise.complete.obs") # Note that the variance-covariance matrix = correlation matrix
carter.model2 <- lm(CARFEEL3 ~ INC + AGE + PARTYID, data=mystandardizeddata) 
summary(carter.model2)

# Now let's create our intercept dummy variables.
mysubsetdata <- subset(mydata, RACE == 1 | RACE == 2) # This gets rid of the "other" category in race.
# The method below creates something called a "factor," and then converts that factor into a real number by adding a zero to it. 
white <- (mysubsetdata$RACE==1)+0
black <- (mysubsetdata$RACE==2)+0
races <- cbind(white,black)
races # Prints out the race data

# Now we create the slope dummy variables and set up our linear regression model.
wpartyid=white*mysubsetdata$PARTYID # This is one way of creating the whites only slope dummy variable for partyid.
bpartyid=black*mysubsetdata$PARTYID # This is one way of creating the blacks only slope dummy variable for partyid.
carter.model <-lm(mysubsetdata$CARFEEL3 ~ white + black + wpartyid + bpartyid + mysubsetdata$SEX + mysubsetdata$AGE - 1)
summary(carter.model)
# Note: R does not calculate the R-squared statistic correctly when suppressing the intercept in the regression model above.
# To do this correctly, the following code is appended here. It will calculate the R-squared statistic for you correctly. 
rss.residuals <- sum((residuals(carter.model))^2)
rss.residuals
mean.dep.var <- mean(mysubsetdata$CARFEEL3, na.rm="TRUE")
mean.dep.var 
tss.dep.var <- sum((mysubsetdata$CARFEEL3-mean.dep.var)^2, na.rm="TRUE")
tss.dep.var 
r.square <- 1 - (rss.residuals/tss.dep.var)
r.square
# One way to test for the equality of two regression parameters is with an F-test, using the Wald procedure.
# Another way is to make a confidence interval. 
# Either way, you will need the coefficient-covariance matrix. Here it is:
V <- vcov(carter.model)
V
